import streamlit as st
import os
from datetime import datetime
from langchain.schema import HumanMessage, AIMessage
from .session import handle_new_chat, load_chat_history, HISTORY_DIR, delete_chat_history

def render_page_config():
    """è®¾ç½®é¡µé¢é…ç½®å’Œæ ‡é¢˜"""
    st.set_page_config(page_title="é‡‘èå’¨è¯¢æ™ºèƒ½ä½“", page_icon="ğŸ’°", layout="wide")
    st.title("ğŸ’° é‡‘èå’¨è¯¢æ™ºèƒ½ä½“")
    st.caption("ç”± LangChain å’Œç«å±±æ–¹èˆŸæ¨¡å‹é©±åŠ¨")

def render_sidebar():
    """æ¸²æŸ“ä¾§è¾¹æ ï¼ŒåŒ…æ‹¬æ–°å¯¹è¯æŒ‰é’®å’Œå†å²è®°å½•"""
    with st.sidebar:
        # æ³¨å…¥è‡ªå®šä¹‰ CSS æ¥è°ƒæ•´æŒ‰é’®å¯¹é½å’Œæ–‡æœ¬å¯¹é½
        st.markdown("""
        <style>
            /* ç¡®ä¿ä¾§è¾¹æ ä¸­çš„åˆ—å‚ç›´å±…ä¸­å¯¹é½ */
            [data-testid="stSidebar"] [data-testid="stHorizontalBlock"] {
                align-items: center;
            }
            /* é’ˆå¯¹å†å²è®°å½•æŒ‰é’®ï¼Œå®ç°æ–‡æœ¬å·¦å¯¹é½ */
            [data-testid="stSidebar"] .stButton button[kind="secondary"] {
                text-align: left;
                justify-content: flex-start;
            }
            /* é’ˆå¯¹åˆ é™¤æŒ‰é’®ï¼Œä¿æŒå±…ä¸­ */
            [data-testid="stSidebar"] .stButton button[kind="secondary"]:has(span[aria-label="wastebasket"]) {
                text-align: center;
                justify-content: center;
            }
        </style>
        """, unsafe_allow_html=True)

        st.header("å¯¹è¯ç®¡ç†")
        if st.button("â• æ–°å¯¹è¯"):
            handle_new_chat()

        st.header("å†å²è®°å½•")
        history_files = sorted(os.listdir(HISTORY_DIR), reverse=True)
        for filename in history_files:
            session_id = filename.split('.')[0]
            col1, col2 = st.columns([0.8, 0.2])
            with col1:
                if st.button(session_id, key=f"history_{filename}", use_container_width=True):
                    load_chat_history(session_id)
            with col2:
                if st.button("ğŸ—‘ï¸", key=f"delete_{filename}", use_container_width=True):
                    delete_chat_history(session_id)
                    st.rerun()

def render_chat_messages(messages):
    """ä»å†å²è®°å½•ä¸­æ˜¾ç¤ºèŠå¤©æ¶ˆæ¯"""
    for message in messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

def render_agent_response(agent, prompt, current_messages):
    """å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œè·å–å¹¶æµå¼æ˜¾ç¤ºæ™ºèƒ½ä½“çš„å›å¤"""
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        status_placeholder = st.empty()
        full_response = ""
        output_started = False
        
        # ä¸ºæ™ºèƒ½ä½“å‡†å¤‡å¯¹è¯å†å²
        chat_history = []
        for msg in current_messages[:-1]: # æ’é™¤åˆšåˆšæ·»åŠ çš„ç”¨æˆ·æœ€æ–°æ¶ˆæ¯
            if msg["role"] == "user":
                chat_history.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                chat_history.append(AIMessage(content=msg["content"]))
        
        # æ³¨å…¥å½“å‰æ—¥æœŸ
        current_date = datetime.now().strftime("%Y-%m-%d")
        enhanced_prompt = f"å½“å‰æ—¥æœŸæ˜¯ {current_date}ã€‚ç”¨æˆ·çš„è¯·æ±‚æ˜¯ï¼š{prompt}"

        try:
            status_placeholder.text("æ€è€ƒä¸­...")
            for chunk in agent.stream({"input": enhanced_prompt, "chat_history": chat_history}):
                if "actions" in chunk and not output_started:
                    for action in chunk["actions"]:
                        status_placeholder.text(f"æŸ¥è¯¢å·¥å…·: {action.tool}...")
                elif "steps" in chunk and not output_started:
                    status_placeholder.text("åˆ†æä¸­...")
                elif "output" in chunk and chunk["output"]:
                    if not output_started:
                        status_placeholder.empty()
                        output_started = True
                    
                    full_response += chunk["output"]
                    message_placeholder.markdown(full_response + "â–Œ")
        finally:
            status_placeholder.empty()
            message_placeholder.markdown(full_response)
            
    return full_response