import streamlit as st
from financial_agent.core.agent import create_financial_agent
from financial_agent.core.llm_adapter import VolcanoLLM
from langchain.schema import HumanMessage, AIMessage

# --- é¡µé¢é…ç½®ä¸æ ‡é¢˜ ---
st.set_page_config(page_title="é‡‘èå’¨è¯¢æ™ºèƒ½ä½“", page_icon="ğŸ’°", layout="wide")
st.title("ğŸ’° é‡‘èå’¨è¯¢æ™ºèƒ½ä½“")
st.caption("ç”± LangChain å’Œç«å±±æ–¹èˆŸæ¨¡å‹é©±åŠ¨")

# --- æ™ºèƒ½ä½“åˆå§‹åŒ– ---
@st.cache_resource
def get_agent():
    llm = VolcanoLLM(streaming=True)
    return create_financial_agent(llm)
agent = get_agent()

# --- ä¼šè¯çŠ¶æ€åˆå§‹åŒ– ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# 1. ä»å†å²è®°å½•ä¸­æ˜¾ç¤ºèŠå¤©æ¶ˆæ¯ (æ ‡å‡†æ¨¡å¼)
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# 2. å“åº”ç”¨æˆ·çš„æ–°è¾“å…¥ (æ ‡å‡†æ¨¡å¼)
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    # a. åœ¨èŠå¤©å®¹å™¨ä¸­æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("user"):
        st.markdown(prompt)
    # b. å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°èŠå¤©è®°å½•
    st.session_state.messages.append({"role": "user", "content": prompt})

    # c. åœ¨èŠå¤©å®¹å™¨ä¸­æ˜¾ç¤ºæ™ºèƒ½ä½“å›å¤
    with st.chat_message("assistant"):
        # åˆ›å»ºç”¨äºæµå¼æ˜¾ç¤ºå†…å®¹çš„å ä½ç¬¦
        message_placeholder = st.empty()
        # åˆ›å»ºç”¨äºæ˜¾ç¤º "æ€è€ƒä¸­..." ç­‰çŠ¶æ€çš„å ä½ç¬¦
        status_placeholder = st.empty()
        
        full_response = ""
        output_started = False
        
        # ä¸ºæ™ºèƒ½ä½“å‡†å¤‡å¯¹è¯å†å² (ä¸åŒ…å«å½“å‰çš„ç”¨æˆ·è¾“å…¥)
        chat_history = []
        for msg in st.session_state.messages[:-1]:
            if msg["role"] == "user":
                chat_history.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                chat_history.append(AIMessage(content=msg["content"]))
        
        try:
            status_placeholder.text("æ€è€ƒä¸­...")
            # ä¼ å…¥ input å’Œ chat_history
            for chunk in agent.stream({"input": prompt, "chat_history": chat_history}):
                if "actions" in chunk and not output_started:
                    for action in chunk["actions"]:
                        status_placeholder.text(f"æŸ¥è¯¢å·¥å…·: {action.tool}...")
                elif "steps" in chunk and not output_started:
                    status_placeholder.text("åˆ†æä¸­...")
                elif "output" in chunk and chunk["output"]:
                    if not output_started:
                        status_placeholder.empty()
                        output_started = True
                    
                    full_response += chunk["output"]
                    message_placeholder.markdown(full_response + "â–Œ") # æ‰“å­—å…‰æ ‡æ•ˆæœ
        finally:
            status_placeholder.empty()
            message_placeholder.markdown(full_response) # æ˜¾ç¤ºæœ€ç»ˆç»“æœ

    # d. å°†å®Œæ•´çš„æ™ºèƒ½ä½“å›å¤æ·»åŠ åˆ°èŠå¤©è®°å½•
    st.session_state.messages.append({"role": "assistant", "content": full_response})